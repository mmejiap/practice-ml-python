{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos Ensamblados - Boosting\n",
    "- Autor: Michael Larico\n",
    "- Módulo 5: Modelos Ensamblados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ÍNDICE\n",
    "\n",
    "1. Introducción a los Modelos Aditivos\n",
    "2. Introducción al Boosting de Regresión\n",
    "3. Intuición sobre el Gradient Boosting\n",
    "4. Ejemplo básico del Gradient Boosting de Regresión\n",
    "5. Medida del rendimiento del Modelo\n",
    "6. Tuning de Hiperparámetros\n",
    "7. Variantes: XGBoost, Adaboost, LightGBM, Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Introducción a los Modelos Aditivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Antes de entrar en detalle sobre los modelos del tipo **Boosting**, veamos un ejemplo de los modelos aditivos debido a que son la base del Boosting. \n",
    "\n",
    "La idea es bastante simple: **Vamos a combinar un conjunto de términos simples para crear una expresión más complicada**. En el mundo de **Machine Learning**, esa expresión (**función**) representa un modelo que correlaciona (o mapea) la característica de alguna observación, x, con un valor objetivo escalar, y.\n",
    "\n",
    "Es una técnica realmente útil porque a menudo podemos interpretar y construir los términos simples más fácilmente que descifrar la función general de una sola vez.\n",
    "\n",
    "Considere la siguiente curva que muestra **y** como alguna función desconocida pero no trivial de x.\n",
    "\n",
    "<img src=\"images/L2-loss_additive_2.svg\" width=\"500\" style=\"margin: 20px auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Supongamos que la función se compone de varios términos simples, por lo que vamos a intentar adivinar cuáles son. Para ello, vamos a agregar cada término y evaluaremos la diferencia entre la función combinada actual y la función objetivo deseada de tal forma que podamos determinar el siguiente término a agregar.\n",
    "\n",
    "Nuestra primera aproximación podría ser la recta horizontal **y = 30** porque podemos ver que la interesección con el eje y (en x=0) es 30. Miremos la primera gráfica en la siguiente secuencia gráfica. Naturalmente, la pendiente es incorrecta, por lo que deberíamos sumarle la recta de 45 grados (con pendiente igual a 1) obteniendo como resultado la segunda gráfica. Además, resulta que la parte ondulada viene de la función **y = sin(x)** por lo que sumandole obtendremos la gráfica de nuestra función objetivo.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/L2-loss_additive_3.svg\" width=\"1200\" style=\"margin: 20px auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Descomponer una función complicada en subfunciones más simples no es más que aplicar la estrategia **\"Divide y vencerás\"** que usamos todo el tiempo cuando queremos resolver un problema complejo. En este caso, estamos dividiendo una función potencialmente muy complicada en subfunciones más simples y manejables:\n",
    "\n",
    "$$F(x) = f_{1}(x) + f_{2}(x) + f_{3}(x)$$, donde:\n",
    "\n",
    "$$f_{1}(x) = 30$$\n",
    "$$f_{2}(x) = x$$\n",
    "$$f_{3}(x) = sin(x)$$\n",
    "\n",
    "De manera más general, los matemáticos describen la descomposición de una función en la adición de una serie de M subfunciones:\n",
    "\n",
    "$$F_{M}(x) = f_{1}(x) + ... + f_{M}(x) = \\sum_{m=1}^{M}f_{m}(x)$$\n",
    "\n",
    "En el mundo de Machine Learning, tenemos un conjunto de datos que utilizaremos para crear una función que dibuje una curva que se ajuste a los puntos. Llamamos a esta función un modelo que asigna un valor y a un conjunto de datos X, de tal forma que hace predicciones dadas algunas observaciones x desconocidas. Agregar un conjunto de subfunciones para crear una función compuesta que modele algunos puntos de datos se denomina **modelo aditivo**.\n",
    "\n",
    "Los modelos de Gradient Boosting usan el modelado aditivo para empujar gradualmente un modelo aproximado hacia un modelo realmento bueno, agregando submodelos simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Introducción al Boosting de Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Boosting es una estrategia poco definida que combina múltiples modelos simples en un solo modelo compuesto. La idea es que, a medida que introducimos modelos más simples, el modelo resultante se convierta en un predictor más fuerte. En la terminología del boosting, los modelos simples se llaman **weak models** o **weak learners**.\n",
    "\n",
    "En el contexto de una regresión, tenemos que hacer predicciones de valores continuos tal como el precio de un alquiler, basados en cierta información tal como el área del inmueble. Para simplicar las cosas, en esta demostración trabajaremos con una sola variable o característica pero, en general, cada observación tiene un vector de características.\n",
    "\n",
    "El entrenamiento de un modelo de regresión es una cuestión de ajustar una función a través de los puntos de datos lo mejor que podamos. Dado un único vector de características y un valor de objetivo escalar **y** para un sola observación, podemos xpresar un modelo compuesto que predice (aproxima) como la adición de M módelos débiles:\n",
    "\n",
    "$$\\hat{y} = \\sum_{m=1}^{M}f_{m}(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los matemáticos representan los modelos débiles y compuestos como funciones, pero en la práctica los modelos pueden ser cualquier modelo tal como los K-vecinos más cercanos o árboles de decisión. Como todos usan árboles para Boosting, nos enfocaremos en las implementaciones que usan árboles de regresión para modelos débiles, lo que también simplifica en gran medida las matemáticas.\n",
    "\n",
    "Más adelante, utilizaremos un vector de N características como filas en una matriz:\n",
    "$$X = [x_{1},x_{2},...,x_{N}]$$\n",
    "\n",
    "y N targets en un vector:\n",
    "$$y = [y_{1},y_{2},...,y_{N}]$$\n",
    "para N observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A menudo ocurre que un modelo aditivo puede construir los términos individuales de forma independiente y en paralelo (bagging), pero ese no es el caso del Boosting. Boosting construye y agrega modelos débiles de forma escalonada, uno después del otro, cada uno elegido para mejorar el rendimiento general del modelo. La estrategia del Boosting es codiciosa en el sentido de que la elección nunca altera las funcionas previas. Podríamos elegir dejar de agregar modelos débiles cuando el rendimiento sea lo suficientemente bueno o cuando no agregue nada. En la práctica, elegimos el número de etapas M como un hiperparámetro del modelo general. Permitir que M crezca arbitrariamente aumenta el riesgo de **overfitting**.\n",
    "\n",
    "Debido a que las estrategias codiciosas eligen un modelo débil a la vez, a menudo verás los modelos Boosting expresados usando esta formulación recursiva equivalente:\n",
    "\n",
    "$$F_{m}(x) = F_{m-1}(x) + F_{m}(x)$$\n",
    "\n",
    "Lo que quiere decir esto es que debemos modificar el modelo anterior para obtener el próximo modelo.\n",
    "\n",
    "Boosting por sí mismo no especifica como elegir a los weak models. Boosting ni siquiera especifica la forma de los modelos, pero la forma del modelo débil dicta la forma del meta-modelo. Por ejemplo, si todos los modelos débiles son modelos lineales, entonces el metamodelo resultante es un modelo lineal simple. Si utilizamos pequeños árboles de regresión como modelos débiles, el resultado es un bosque de árboles cuyas predicciones se suman.\n",
    "\n",
    "Veamos si podemos diseñar una estrategia para elegir modelos débiles para crear nuestro propio algoritmo de Boosting para una sola observación. Entonces, podemos extenderlo para trabajar en las múltiples observaciones que encontraríamos en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Intuición sobre el Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para construir un modelo Boosting de regresión, comencemos creando un modelo malo, $$f_{0}(x),$$que prediga una aproximación inicial de **y** dado un vector de características. Luego, empujemos gradualmente el modelo general $$F_{m}(x)$$ hacia el valor objetivo conocido \"y\" agregando uno o más ajustes, $$\\triangle_{m}(x):$$\n",
    "\n",
    "$$\\hat{y} = f_{0}(x) + \\triangle_{1}(x) + \\triangle_{2}(x) + ... + + \\triangle_{M}(x)$$\n",
    "$$\\hat{y} = f_{0}(x) + \\sum_{m=1}^{M}\\triangle_{m}(x)$$\n",
    "$$\\hat{y} = F_{M}(x)$$\n",
    "\n",
    "O, usando una relación de recurrencia:\n",
    "\n",
    "$$F_{0}(x) = f_{0}(x)$$\n",
    "$$F_{m}(x) = F_{m-1}(x) + \\triangle_{m}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podría ser útil pensar en este enfoque de Boosting como un golfista que golpea inicialmente una pelota de golf hacia el hoyo **y** pero llega muy lejos como $$f_{0}(x).$$ Luego, el golfista golpea repetidamente la pelota más suavemente, tratando de acercar la pelota hacia el hoyo, luego de volver a evaluar la dirección y la distancia hacia el hoyo en cada etapa. El siguiente diagrama ilustra los 5 golpes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: We detected that you are running inside a Ipython/Jupyter notebook environment but we cannot save your notebook source code. Please be sure to have installed comet_ml as a notebook server extension by running:\n",
      "jupyter comet_ml enable \n",
      "For more details, please refer to: https://www.comet.ml/docs/python-sdk/warnings-errors\n",
      "COMET WARNING: Comet.ml support for Ipython Notebook is limited at the moment, automatic monitoring and stdout capturing is deactivated \n",
      "For more details, please refer to: https://www.comet.ml/docs/python-sdk/warnings-errors\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/mlaricobar/general/c1fb21a5e8704b7bb63b751a7e79f96f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(api_key=\"ULkatyRghlUtgkfrkIHaFVIsa\", project_name=\"general\", workspace=\"mlaricobar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer.keys(): dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n",
      "Shape of cancer data: (569, 30)\n",
      "\n",
      "Sample counts per class:\n",
      "{'malignant': 212, 'benign': 357}\n",
      "\n",
      "Feature names:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "\n",
      "Results\n",
      "Confusion matrix \n",
      " [[52  1]\n",
      " [ 1 89]]\n",
      "F1 score is  0.989\n",
      "Precision score is  0.989\n",
      "Recall score is  0.989\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "random_state = 42\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys(): {}\".format(cancer.keys()))\n",
    "print(\"Shape of cancer data: {}\\n\".format(cancer.data.shape))\n",
    "print(\"Sample counts per class:\\n{}\".format(\n",
    "      {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\n",
    "print(\"\\nFeature names:\\n{}\".format(cancer.feature_names))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data,\n",
    "    cancer.target,\n",
    "    stratify=cancer.target,\n",
    "    random_state=random_state)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,5,10,20,50,100]}\n",
    "\n",
    "clf = GridSearchCV(logreg,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=10,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nResults\\nConfusion matrix \\n {}\".format(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1 score is {:6.3f}\".format(f1))\n",
    "print(\"Precision score is {:6.3f}\".format(precision))\n",
    "print(\"Recall score is {:6.3f}\".format(recall))\n",
    "\n",
    "#these will be logged to your sklearn-demos project on Comet.ml\n",
    "params={\"random_state\":random_state,\n",
    "        \"model_type\":\"logreg\",\n",
    "        \"scaler\":\"standard scaler\",\n",
    "        \"param_grid\":str(param_grid),\n",
    "        \"stratify\":True\n",
    "}\n",
    "\n",
    "metrics = {\"f1\":f1,\n",
    "\"recall\":recall,\n",
    "\"precision\":precision\n",
    "}\n",
    "\n",
    "exp.log_dataset_hash(X_train_scaled)\n",
    "exp.log_multiple_params(params)\n",
    "exp.log_multiple_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.65909581,  0.21720545,  1.6106199 , ...,  0.70183483,\n",
       "        -0.55608415,  0.38878074],\n",
       "       [-0.33816539, -1.38996798, -0.40166719, ..., -0.90060625,\n",
       "        -0.92364564, -0.79723299],\n",
       "       [ 0.87445748, -0.65165862,  1.01036969, ...,  2.09675055,\n",
       "         1.76721075,  1.16521656],\n",
       "       ...,\n",
       "       [ 0.39511479,  1.04780348,  0.50533158, ...,  1.52062207,\n",
       "         0.15453473,  1.20793655],\n",
       "       [ 0.84877841, -0.04840585,  0.90273861, ...,  2.08632962,\n",
       "         0.30155932,  0.34873075],\n",
       "       [-1.22637597, -0.51885297, -1.20434661, ..., -0.9053701 ,\n",
       "        -0.58058825,  0.03206882]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = \"C:\\\\Users\\\\S73984\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Library\\\\envs\\\\base\\\\etc\\\\jupyter\\\\nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "cm.update(\"livereveal\", {\n",
    "              \"theme\": \"sky\",\n",
    "              \"transition\": \"zoom\",\n",
    "              \"start_slideshow_at\": \"selected\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "livereveal": {
   "enable_chalkboard": true,
   "scroll": true,
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
